\chapter{System Development}
\label{chp:project}

\section{System Architecture Design}
The design of the system was a crucial part of my internship, consuming approximately 20\% of the total hours dedicated to the project. It was crafted to meet the specific requirements of the business while also taking into account the current context and future needs of the company. UNOX, having collaborated with \ac{AWS} since 2019, leverages innovative \ac{AWS} technologies to maintain a competitive edge. For this project, we were guided by an \ac{AWS} Solution Architect and an Enterprise Account Manager, who played an essential role in helping us build a cutting-edge system that adheres to \ac{AWS}'s Well-Architected Framework principles \cite{awswell}, ensuring Operational Excellence, Reliability, Performance Efficiency, Security, Cost Optimization, and Sustainability.

The \ac{AWS} Solution Architect, in particular, provided detailed comparisons of the various \ac{AWS} technologies available, enabling key decision-makers such as the team leader, the company's \ac{CIO}, and myself to gain a comprehensive understanding of the potential architectures we could build. During the early stages of my internship, we held recurring meetings to explore potential solutions that best aligned with our specific requirements and the nature of UNOX's business operations. These discussions helped us identify the ideal path to follow, although the initial solutions inevitably evolved as we encountered and faced practical challenges during the system's development.

A core aspect of the system design was to ensure a clear separation between the storage layer and the business analytics layer, effectively decoupling data producers (such as operational systems) from data consumers (like reporting and predictive analytics systems). This separation was essential to facilitate data science activities, where a data lake provides a convenient storage layer for experimental data, supporting both the input and output of data analysis tasks. The architecture also needed to support autonomous creation and use of data, without the need for coordination between programs or analysts, while at the same time enabling the sharing and reuse of massive datasets through a distributed computational framework.

\subsection{Data Lake vs. Data Warehouse vs. Data Lakehouse}
The initial idea for this project was to implement a data lake, a more innovative and flexible approach compared to traditional data warehouses. While both are used for storing large volumes of data, they serve different purposes and have distinct architectural characteristics.

Traditionally, a \textbf{data warehouse} is optimized for structured data, meaning data is cleaned, organized, and stored in a predefined schema, making it ideal for business reporting and analytics. However, data warehouses typically involve high setup and maintenance costs, and they require significant preprocessing to ensure data consistency before it can be used for analysis. Hence, they suffer from limited flexibility for advanced analytics, including machine learning tasks \cite{bdcc6040132}.

A \textbf{data lake}, on the other hand, offers a more flexible storage solution. It is capable of storing vast amounts of both structured and unstructured data in its raw form, allowing for greater adaptability. This means that data lakes are not bound by rigid schemas and can accommodate data from diverse sources without the need for heavy preprocessing. Data lakes are particularly suitable for data science, machine learning, and exploratory analysis, as they allow analysts and data scientists to directly interact with raw data, creating an environment where experimentation can thrive. 

\begin{table}[h!]
\centering
\begin{tabular}{|p{2.5cm}|p{6.25cm}|p{6.25cm}|}
\hline
\textbf{Parameters}        & \textbf{Data Warehouse}                                      & \textbf{Data Lake}  \\ \hline
\textbf{Data}              & Focuses only on business processes                           & Stores everything   \\ \hline
\textbf{Processing}        & Highly processed data                                        & Mainly unprocessed data                                     \\ \hline
\textbf{Type of Data}      & Mostly in tabular form and structured                        & Can be unstructured, semi-structured, or structured  \\ \hline
\textbf{Task}              & Optimized for data retrieval                                 & Share data stewardship  \\ \hline
\textbf{Agility}           & Less agile, has a fixed configuration                        & Highly agile, can be configured and reconfigured as needed \\ \hline
\textbf{Users}             & Widely used by business professionals and business analysts  & Used by data scientists, data developers, and business analysts  \\ \hline
\textbf{Storage}           & Expensive storage for fast response times                    & Designed for low-cost storage                                       \\ \hline
\textbf{Security}          & Allows better control of the data                            & Offers less control  \\ \hline
\textbf{Schema}            & Schema on writing (predefined schemas)                       & Schema on reading (no predefined schemas)                           \\ \hline
\textbf{Data Processing}   & Time-consuming to introduce new content                      & Helps with fast ingestion of new data                               \\ \hline
\textbf{Data Granularity}  & Data at the summary or aggregated level of detail            & Data at a low level of detail or granularity                        \\ \hline
\textbf{Tools}             & Mostly commercial tools                                      & Can use open-source tools such as Hadoop or MapReduce             \\ \hline
\end{tabular}
\caption{Comparison between Data Warehouse and Data Lake}
\label{tab:dataw}
\end{table}

It enables autonomous data handling, and data producers do not need to coordinate directly with consumers. It also provides a shared storage framework, which facilitates collaboration between teams and allows for the re-use of large datasets without duplication or complex integration. A detailed analysis of the differences between data warehouses and data lakes is given in Table \ref{tab:dataw} \cite{nambiar2022overview}.

However, while data lakes excel in flexibility, they can sometimes suffer from challenges related to data governance, data quality, performance, and metadata management. As a result, organizations have adopted a two-tier architecture: storing data in lakes and then moving curated data to warehouses for structured analytics.
The two-tier model (data lakes + data warehouses) introduces new complexities, including reliability and cost issues. Indeed, maintaining consistency between the lake and warehouse is complex, storing data in two places and running \ac{ETL} processes increases costs.

This is where a more modern architecture, the \textbf{Data Lakehouse} \cite{armbrust2021lakehouse}, promoted by Databricks\footnote{\url{https://www.databricks.com/}}, a cloud platform built on Apache Spark that enables unified data analytics, machine learning, and big data processing, comes into play.

The lakehouse architecture combines the best features of both data lakes and data warehouses. It retains the ability of a data lake to store raw and semi-structured data while incorporating some of the data management and performance optimization features of a data warehouse. This hybrid approach allows for real-time analytics and \ac{ACID} transactions on large datasets by adding structured layers of metadata to the raw data.
Actually, although we call it a data lake for simplicity, the system developed in this project can be interpreted as a data lakehouse. This is because we have integrated several layers that provide pre-aggregated tables in Parquet or Iceberg formats, which are directly usable for advanced analysis. These formats not only offer significant performance benefits through better compression and faster query times but also enable \ac{ACID} operations. This structured approach allows us to maximize the system's potential for advanced business intelligence while maintaining the flexibility and scalability inherent in a data lake.

\subsection{System architecture overview}
\label{sec:wholesystem}

The architecture of the entire system, as illustrated in Figure \ref{fig:wholesystem}, was designed to meet the company's specific requirements, leveraging cloud technologies to handle large volumes of data while ensuring performance, reliability, and cost-efficiency. This event-driven design follows a predefined pipeline, where each event triggers the execution of a specific Amazon service with tailored parameters. The entire flow is designed to run at a defined frequency, ensuring up-to-date data availability for analysis. The solution automates the process of ingesting, transforming, and analyzing data from various sources, providing a centralized platform for data storage and business intelligence.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.55\textwidth]{res/datalake-system.pdf}
    \caption{The System Architecture}
    \label{fig:wholesystem}
\end{figure}

The system starts with data being collected from two main sources: PostgreSQL for operational data and MongoDB for \ac{IoT} data generated by the ovens. Each of these data sources follows a custom extraction process. For PostgreSQL, \ac{AWS} Glue is used to execute \ac{ETL} jobs that extract the data, convert it into optimized formats, and load it into Amazon \ac{S3}. For MongoDB, the system uses \ac{AWS} Lambda to manage event-driven data extraction, process the data and store it in \ac{S3} in a scalable and efficient way.

Once the data is in \ac{S3}, it is organized into three layers:
\begin{itemize}
    \item \textbf{Raw}: Where the data is stored as extracted, without any transformation.
    \item \textbf{Curated}: Where the data is cleaned, formatted, and partitioned for better query performance.
    \item \textbf{Analytics}: Where the data is pre-aggregated and optimized for specific use cases, such as business reports.
\end{itemize}
The system relies on \ac{AWS} Glue Data Catalog to manage metadata, enabling easy access and query capabilities. For queries, \ac{AWS} Athena is used to allow \ac{SQL} queries directly on the data stored in \ac{S3}, while \ac{AWS} QuickSight provides interactive dashboards and visualizations for business users to explore and analyze the up-to-date data.

To enhance security, each \ac{AWS} service involved in the event-driven pipeline has tailored \ac{IAM} policies. These policies restrict access so that each service can only interact with the specific resources it requires, minimizing potential exposure to other parts of the Amazon ecosystem. The system utilizes stringent access rules to reinforce security by ensuring only the minimum necessary permissions are granted for each \ac{AWS} event, helping protect sensitive data and maintaining a secure environment.

In the following sections, a detailed description of the entire workflow will be provided, including how data ingestion, integration, and cataloguing are performed, as well as how queries and reports are generated. This chapter will also cover how orchestration and scheduling are managed through \ac{AWS} Step Functions, ensuring that each component of the system works seamlessly and in the correct sequence.


\section{Data Sources}
Before going into the implementation details of the solution, it is necessary to analyse the two data sources in order to better understand certain choices made during the design phase.
\subsection{PostgreSQL Data}
\label{sec:postgresdata}
The Postgres database in this system contains crucial operational data, including various datasets related to Unox ovens and their usage. It is managed by an \ac{AWS} \ac{RDS} instance (described in section \ref{sec:rds}) that uses a \texttt{db.t3.xlarge} configuration, providing 4 vCPUs, 16 GB \ac{RAM}, and 100 GiB of gp2 storage. The database runs PostgreSQL version 12.19, and backups are automatically created every 14 days to ensure data safety.

PostgreSQL hosts several databases, among which the most heavily used is the \texttt{ddc} database. The name \texttt{ddc} refers to the \acf{DDC} platform, an intelligent cooking system, developed by the Digital Experience team, that leverages data to optimize and enhance cooking processes. \ac{DDC} offers advanced features for oven owners, enabling them to efficiently monitor and control their devices.

The \texttt{ddc} database comprises 72 tables, with the most critical ones being:
\begin{itemize}
    \item \textbf{Device}: Contains detailed information about all network-connected devices produced by Unox.
    \item \textbf{Company}: Stores information regarding the companies that own Unox ovens.
    \item \textbf{Device group}: Facilitates grouping of devices within a company, allowing management differentiation based on factors such as location, model, or other criteria.
    \item \textbf{Device recipe}: Tracks the current recipe loaded on a device.
\end{itemize}
The relational schema of the main tables is shown in the figure \ref{fig:relschema}. In this diagram, diamonds represent relationship tables, while arrows indicate foreign key relationships pointing to the primary key of the referenced table.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{res/db-schema.pdf}
    \caption{Main tables relational schema}
    \label{fig:relschema}
\end{figure}
As seen in the diagram, recipes related to a company can also be differentiated based on the device group. This means that recipes can apply at different levels: specific to a device, to a group of devices, or to an entire company. Furthermore, recipes can be created by the community or by Unox itself, stored in the \texttt{community\_recipe} and \texttt{chefunox\_recipe} tables, respectively.

In addition to these, there are other tables related to the primary ones, such as those containing data on the parameters, profiles, or settings of a company or device.

Two more tables, \texttt{device\_recipe\_history} and \texttt{device\_ip\_info\_history}, contain historical data. The former records all recipes created since the installation of a device, while the latter stores the history of IP addresses and associated information for a device. These are the largest tables in terms of storage, with \texttt{device\_recipe\_history} occupying 22 GB and \texttt{device\_ip\_info\_history} storing 5 GB of data.

\subsection{MongoDB Data}
Since \ac{AWS} \ac{RDS} does not support the MongoDB engine, this database is installed on an \ac{AWS} \ac{EC2} machine. The \ac{EC2} instance used is a \texttt{c5.4xlarge}, with 16 vCPUs and 32 GiB of \ac{RAM}. The C5 instances are optimized for compute-intensive workloads and offer high performance at a low cost, providing an optimal balance between price and computational power.

Unfortunately, the MongoDB database is deployed on a single replica set and is not configured as a \textit{sharded} cluster. A replica set consists of a group of MongoDB instances that maintain the same dataset, providing redundancy and high availability. Furthermore, without a sharded cluster, the system cannot horizontally scale across multiple nodes, which limits its ability to handle large-scale datasets and high read/write throughput efficiently.

MongoDB organizes data into collections and documents, which can be thought of as equivalent to tables and rows in a relational database, respectively. A document in MongoDB is a flexible, schema-less structure that can be represented as a \ac{JSON}-like object, where there are no constraints on the data types or mandatory fields.

The database contains 20 collections, of which 13 have been deemed useful for analysis and inclusion in the data lake. Table \ref{tab:sizes} describes all the collections, including the approximate number of documents and the storage space they occupy.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|r|r|r|}
    \hline
    \textbf{Collection} & \multicolumn{1}{l|}{\textbf{Total \# of}} & \multicolumn{1}{l|}{\textbf{Size}} & \multicolumn{1}{l|}{\textbf{Storage Size}} \\ 
                        & \multicolumn{1}{l|}{\textbf{Documents}}   &  &          \\ \hline
    \texttt{alarm}                & 43 M    & 3.7 GiB & 1.9 GiB \\ \hline
    \texttt{end\_of\_prog}        & 240 M   & 60 GiB & 24.7 GiB \\ \hline
    \texttt{end\_of\_prog\_aggregated} & 29 M    & 16.2 GiB & 15.8 GiB \\ \hline
    \texttt{events}               & 1500 M  & 152.6 GiB & 78.2 GiB \\ \hline
    \texttt{evereo\_sess}         & 215 K   & 48.4 MiB & 20 MiB \\ \hline
    \texttt{request}              & 820 K   & 60.7 GiB & 38.8 GiB \\ \hline
    \texttt{sd\_events}           & 75 M    & 11.2 GiB & 5.2 GiB \\ \hline
    \texttt{sd\_haccp}            & 220 M   & 90.3 GiB & 23.6 GiB \\ \hline
    \texttt{sd\_messages}         & 220 M   & 130 GiB & 25.3 GiB \\ \hline
    \texttt{sd\_variables}        & 190 M   & 505 GiB & 156.2 GiB \\ \hline
    \texttt{sdata}                & 170 K   & 80.6 MiB & 29 MiB \\ \hline
    \texttt{variable\_logs}       & 970 M   & 7.3 TiB & 1.8 TiB \\ \hline
    \texttt{working\_minutes\_logs} & 180 K  & 29.3 MiB & 8.6 MiB \\ \hline
    \end{tabular}
    \caption{MongoDB collections used for analysis, with document counts and storage sizes.}
    \label{tab:sizes}
\end{table}

All the collections store time series data, which means that they record sequences of data points indexed in time order. Consequently, each collection includes an \texttt{idDevice} field, indicating which device produced the specific document, and a timestamp field that records when the data was generated. In addition to these, each collection contains specific fields that characterize its data.

As we specified earlier, the data belonging to these collections are generated by the various sensors installed in the devices. When the device is connected to the internet and ready for transmission, the collected data is sent to the back-end, which is responsible for uploading it to the database. The back-end implements a retention mechanism, ensuring reliability in case of database unavailability or upload issues. If a problem occurs, the device is alerted that the data upload was unsuccessful, meaning the device must reattempt the upload once it is ready to transmit again. This mechanism provides robustness, preventing data gaps and duplicates across MongoDB collections. Each device, therefore, uploads data independently, with unpredictable upload intervals.

After successfully uploading the documents to MongoDB, the system updates the \texttt{last\_log\_date} timestamp in the \texttt{device} table of the \ac{DDC} database. This field reflects the timestamp of the most recent document uploaded for the respective device. Observing this \texttt{last\_log\_date} column, we can confirm that all data with earlier timestamps has been successfully uploaded. However, there may still be additional data recorded by the device that has not yet been uploaded to MongoDB.

It is worth noting that the \texttt{variable\_logs} collection is the largest in terms of data size and will be described in detail in the following section.
\subsubsection{Variable Logs}
The \texttt{variable\_logs} collection represents a time series that captures telemetry data from the ovens. Every 30 seconds, each oven sends data for every available sensor. Each sensor is represented by a variable, and there are sensors that measure various parameters, such as temperature, humidity, fan speed, microwave activity, and other values related to the engine or power supply. For each of these groups, multiple sensors may exist. For example, temperature can be measured in several locations, including the cooking chamber, the core probe, the control board, or the power board. There may also be temperature values such as the one set by the user or the one recommended by algorithms. Additionally, measurements may be sent by accessories connected to the oven.

Each oven can send up to 30-35 measurements every 30 seconds to the back-end, which handles the writing to the database. However, only some of these measurements are actually sent, depending on the oven family, model, and the type of program that is running.

Unfortunately, the document structure in this collection is not very intuitive. Each document represents a 6-hour slot of measurements for a single sensor, divided into 6 one-hour samples, with each sample containing 120 measurements (one every 30 seconds). This format is optimized for the oven's internal algorithms and is maintained for backward compatibility.

From an analytical perspective, this pattern is somewhat inconvenient. When querying data for specific time periods, a single query returns 720 measurements, and to locate a precise measurement, one must navigate through the nested structure of the \ac{JSON} document.

To address this issue, a view of the collection was previously created, called \texttt{variable\_logs\_clean}, using an aggregation pipeline composed of 7 stages. This view transforms the data structure into a more accessible format by unpacking each document so that there is a single document for each individual measurement, associating it with the specific timestamp of that measurement. Non-existent measurements are excluded from the view, thus preventing the creation of unnecessary documents. This solution results in a significant increase in the number of documents, but each document is much lighter in terms of storage.

For convenience, in the data lake, the view \texttt{variable\_logs\_clean} will be used as the source for oven telemetry data instead of the original collection.

\section{Data Ingestion}
As outlined in the introduction, the data ingestion process differs depending on the two main data sources used in the system: PostgreSQL and MongoDB. These differences arise from the unique requirements of each database and the technologies used to connect and extract data.

For PostgreSQL, \ac{AWS} Glue establishes a connection using \ac{JDBC}. \ac{JDBC} is a standard \ac{API} that facilitates communication between a client and a relational database, enabling Glue to execute queries, extract data, and transform it for storage in a structured format like Parquet \cite{glueingestion}. This process is crucial for handling structured data efficiently.

In contrast, for MongoDB, the ingestion process involves a direct connection using a MongoDB-specific driver. This type of connection allows the system to interact with MongoDB. Unlike \ac{JDBC}, the driver is tailored for the unique characteristics of MongoDB, allowing for the handling of unstructured or semi-structured data.

Due to the need for multiple sequential jobs to export data effectively, it is essential to be able to select specific data subsets for each job. In PostgreSQL, \ac{AWS} Glue jobs handle this through incremental data ingestion allowed by Bookmarks, while for MongoDB, a different approach was required. To achieve scalability and flexibility in handling the unstructured data from MongoDB, \ac{AWS} Lambda was chosen as the ingestion tool.

\subsection{PostgreSQL Data}
\subsubsection{Connection to the database}
Before building the actual extraction job for PostgreSQL, it was essential to configure the connection to the database. \ac{AWS} Glue facilitates the management of connections through a dedicated section where various parameters can be defined. However, prior to configuring this, it was necessary to analyze the company's networking architecture on \ac{AWS}.

In \ac{AWS}, networking is managed through a \ac{VPC}, which is a virtual network dedicated to an \ac{AWS} account. A \ac{VPC} can contain multiple subnets, which are smaller segments of the \ac{VPC} used to organize and isolate resources within different parts of the network. Subnets can be public or private, depending on whether they are associated with an Internet Gateway (IGW) that allows communication with the Internet. Security Groups act as virtual firewalls, controlling inbound and outbound traffic for \ac{AWS} resources. Routing Tables are used to manage the paths that data packets take to reach various destinations, such as other \ac{AWS} resources or external networks via the Internet Gateway.

In this specific setup, the PostgreSQL \ac{RDS} instance is distributed across three subnets, all of which belong to the same \ac{VPC}. These subnets share a single routing table that contains an Internet Gateway, allowing communication with external networks, including the Internet.

To configure the connection to the \ac{RDS} instance, several parameters had to be specified:
\begin{itemize}
    \item \textbf{Database credentials}: This includes the connection type, host, username, password, and port number.
    \item \textbf{Networking details}: The \ac{VPC}, subnet, and security group had to be defined to ensure that Glue could securely connect to the \ac{RDS} instance.
\end{itemize}
Once these were set up, it was necessary to create a \ac{VPC} Endpoint in the routing table. A \ac{VPC} endpoint enables a private connection between \ac{AWS} services (in this case, \ac{RDS} and Glue) without the need to traverse the public internet. In this context, the \ac{VPC} Endpoint was crucial for allowing Glue to access the \ac{RDS} instance directly and securely within the \ac{VPC}. This reduces latency and enhances security by keeping traffic within \ac{AWS}'s private network.

\subsubsection{ETL Job description}
\label{sec:gluejobdesc}
After setting up the connection to the PostgreSQL database, the next step was to create the actual \ac{ETL} (Extract, Transform, Load) job for data extraction. \ac{AWS} Glue provides a powerful environment for automating \ac{ETL} workflows, and each Glue job operates as a script written in either Python or Scala, leveraging Apache Spark as the underlying engine for distributed data processing.

The script can be generated visually or written manually, and it is executed in a serverless environment, meaning there is no need to manage the infrastructure, as Glue handles the allocation of resources dynamically. The \ac{ETL} script operates using a GlueContext, a specialized context that integrates \ac{AWS} Glue-specific features into Spark's ecosystem. GlueContext provides the necessary methods to read from and write to various data sources, like databases, \ac{S3} buckets, and data catalogs.

Data in Glue is stored in a structure known as a DynamicFrame \cite{dyframe}, which is similar to Spark's DataFrame but with added flexibility for semi-structured or schema-less data. A key difference between a DynamicFrame and a DataFrame lies in the level of schema enforcement. While a DataFrame in Spark is strictly tied to a predefined schema, a DynamicFrame is schema-aware but more flexible, allowing it to adapt to evolving data structures. DynamicFrames are especially useful in \ac{ETL} processes that involve reading from semi-structured sources where data may not adhere to a rigid schema.

As outlined in section \ref{sec:glue}, \ac{AWS} Glue simplifies the creation of \ac{ETL} jobs through a fully visual interface. Glue jobs are structured as workflows, which consist of multiple steps that can be executed either sequentially or in parallel. The visual interface allows users to design a flowchart, where each node represents a specific step in the job. These nodes are categorized as Source, Transformation, or Target, depending on the role they play in the \ac{ETL} process.

For example, in the simplest case of moving a dataset from one location to another, only two nodes are required: a Source node to define the data source and a Target node to specify the destination. Additional parameters such as bookmark usage (to track job progress), execution type, timeout settings, and the database connection are configured globally at the job level. \ac{AWS} Glue automatically transforms the visual \ac{ETL} workflow into a Python or Scala script. Therefore, each time the job is executed, it runs the script that was generated based on the visual design. Each node in the flowchart corresponds to a function call, either from Apache Spark or from \ac{AWS} Glue's specific libraries.

While visual \ac{ETL} tools are convenient and intuitive for building basic workflows, they have limitations. They do not always expose all the parameters or advanced options available in the GlueContext or SparkContext. This is particularly restrictive when more granular control is needed, such as fine-tuning the performance of data extraction or transformation steps. For this reason, a custom Python script was written for the PostgreSQL extraction. This approach allowed for full utilization of Glue's and Spark's capabilities, ensuring optimal performance and flexibility. Using a custom script also allowed for more advanced data filtering, error handling, and optimization techniques.


\subsubsection{Glue Bookmarking in JDBC Sources}

In the context of \ac{AWS} Glue, \textit{bookmarks} serve as a mechanism to track the progress of a job by saving the last processed primary key. Specifically, for \ac{JDBC} sources, the bookmark stores the value of the last primary key that was successfully processed during the job execution. This information is stored in an internal \ac{JSON} file in Glue's storage system. When the job is run again, it checks whether a bookmark is present for each table. If a bookmark exists, the job filters the data by selecting only those rows where the primary key is greater than the one saved in the bookmark, ensuring that only new rows are imported.

However, this functionality presents certain limitations. If a row in the source table is updated, but its primary key remains unchanged, the updated row will not be selected during the next job execution, leading to outdated data in the data lake. On the other hand, if the primary key changes during the update, the updated row is imported into the data lake, but the old version of the row remains, resulting in duplicated data. Consequently, the use of bookmarks is only advantageous in cases where the source table is append-only, meaning no updates or deletions are performed.

Given these constraints, specific solutions have been proposed depending on the behavior of the tables in the company's \texttt{ddc} database:
\begin{itemize}
    \item For tables that only undergo append operations, the use of bookmarks is feasible, allowing the job to load only the newly inserted rows. This helps optimize the data ingestion process.
    \item For tables that undergo upserts (i.e., updates or inserts), a more efficient solution is to replace the entire table during each job execution. This involves deleting the previous version of the table in the data lake and replacing it with the updated version from the source. Since the total memory occupied by these tables is less than 300 MB, this approach is manageable from a performance and cost perspective.
    \item A special case is the \texttt{device\_recipe} table, which deletes a row and reinserts it with a new primary key whenever an update occurs. Given the size of this table (approximately 1 GB), the most effective solution is to replace the entire table in the data lake after each job execution to ensure data consistency.
    \item The \texttt{device} table also behaves similarly to \texttt{device\_recipe}, but with an additional requirement for \textit{Time Travel}. Time Travel is a feature that would be useful in this context to retain historical data, such as older firmware versions installed on the devices or previous IP addresses. In this case, the use of Apache Iceberg is proposed as a solution. Iceberg is a table format designed for large-scale datasets, providing capabilities such as schema evolution, partitioning, and Time Travel. During each job execution, the entire \texttt{device} table (around 70 MB) would be loaded, and, if the table is already present in the data lake, a \texttt{MERGE INTO} operation would be performed.
\end{itemize}
The \texttt{MERGE INTO} operation in Iceberg works by combining data from two tables based on a matching condition. Specifically, it checks for rows that already exist in the target table and updates them with the new data from the source. If a row in the source table does not have a matching row in the target table, it is inserted as a new row. This process ensures that both the updated and new rows are correctly handled without creating duplicates.

While performing a \texttt{MERGE} between two nearly identical tables (with only a few updated rows) is not the most efficient operation, a proposed optimization is to load only the rows with a recent \texttt{updated\_at} timestamp. This column, which indicates the time at which a row was last updated, can be used to filter rows that have been updated within a certain time frame. For example, a query like \texttt{SELECT * FROM device WHERE updated\_at >= NOW() - INTERVAL '2 DAYS'} would select only the rows that have been updated in the last two days, reducing the amount of data that needs to be processed during the \texttt{MERGE INTO} operation.

However, due to inconsistencies in the \texttt{updated\_at} column, it was ultimately decided to perform the \texttt{MERGE} with the entire table as the source, ensuring that no updates are missed.

Before diving into the specifics of the implemented script, it is worth mentioning that the entire solution could have been replaced by a \ac{CDC} system \cite{cdc}, for example using \ac{AWS} \ac{DMS}. \ac{AWS} \ac{DMS} allows for continuous replication of data changes from a source database to a target location, capturing inserts, updates, and deletes in real-time. By leveraging \ac{CDC}, it would be possible to automatically detect and replicate any change in the PostgreSQL database to the data lake, eliminating the need for periodic full-table exports. However, this solution was discarded due to the high costs associated with its constant execution, as \ac{DMS} would require a continuous running process to capture all changes.

\subsubsection{Glue Script}

The exporting code is divided into three distinct phases, each processing a subset of tables sequentially based on their characteristics and behavior. Below is a detailed explanation of each part of the script.

\paragraph{First Phase: Processing Upsert Tables}

The first part of the script processes tables that can undergo upsert operations (update or insert). The list \texttt{tableNames} contains the name of tables to be processed, which was generated by querying the \texttt{information\_schema.tables}, a system table that holds metadata about all tables in the database. From this set, append-only tables were excluded.

The following code snippet illustrates how each table is processed:

\begin{lstlisting}[language=Python, caption=First phase Postgres extraction]
for tableName in tableNames:
    logger.info("TABLE: " + str(tableName))
    table = glueContext.create_dynamic_frame.from_options(
        connection_type = "postgresql",
        connection_options = {
            "useConnectionProperties": "true",
            "dbtable": tableName,
            "connectionName": "Postgresql connection production",
        }
    )
    
    objects_to_delete = s3.list_objects_v2(Bucket="datalake-postgres", Prefix=tableName+"/")
    if 'Contents' in objects_to_delete:
        delete_keys = {'Objects': [{'Key': obj['Key']} for obj in objects_to_delete['Contents']]}
        s3.delete_objects(Bucket="datalake-postgres", Delete=delete_keys)

    out = glueContext.getSink(path="s3://datalake-postgres/"+tableName+"/", connection_type="s3", updateBehavior="UPDATE_IN_DATABASE", partitionKeys=[], enableUpdateCatalog=True, transformation_ctx="write_"+tableName)
    out.setCatalogInfo(catalogDatabase="postgres", catalogTableName=tableName)
    out.setFormat("glueparquet", compression="snappy")
    out.writeFrame(table)
\end{lstlisting}

The \texttt{glueContext.create\_dynamic\_frame.from\_options()} function reads the PostgreSQL table and converts it into a \texttt{DynamicFrame}, which can then be manipulated and written to other destinations. Since these tables undergo upserts, it is necessary to delete the existing tables in Amazon \ac{S3} to replace them with the updated version. This is done using the \texttt{list\_objects\_v2()} and \texttt{delete\_objects()} methods from the \texttt{boto3} client. \texttt{boto3} is the \ac{AWS} \ac{SDK} for Python, which allows to interact with \ac{AWS} services, such as \ac{S3}.

After clearing the existing data, the updated \texttt{DynamicFrame} is written back to \ac{S3} in Parquet format using \texttt{glueContext.writeFrame()}. The data is automatically added to the \ac{AWS} Glue Data Catalog, making it available for further querying and processing. The use of the \texttt{Snappy} compression format ensures efficient storage.

\paragraph{Second Phase: Processing Append-Only Tables}

The second phase processes tables that are append-only, meaning that new rows are only added, and no updates or deletions occur. This phase is similar to the first one, but with a few key differences. First, in this case, the use of bookmarks is enabled, allowing the job to only load new rows since the last execution. This is achieved by configuring the \texttt{transformation\_ctx} parameter, which ensures the bookmark functionality tracks the last processed row and continues from that point during the next run.

Furthermore, since some of these append-only tables (as discussed in section \ref{sec:postgresdata}) contain a large amount of data, it was necessary to optimize the read operations. This was done by using the \texttt{hashfield} and \texttt{hashpartition} parameters. These parameters enable partitioning of the data based on a hash of the specified field, which allows for parallel processing, improving the performance of reading large tables. The \texttt{hashfield} determines the column used for hashing, and \texttt{hashpartition} defines how many partitions the data should be split into for parallel execution.

\paragraph{Third Phase: Processing the Device Table with Time Travel}

The third phase of the script processes the \texttt{device} table, which involves upsert operations and requires \textit{Time Travel}. To enable Time Travel, the table is stored in Apache Iceberg format, which supports versioning and allows efficient retrieval of data from different historical snapshots.

Below is the snippet that handles the \texttt{MERGE INTO} operation for the \texttt{device} table:

\begin{lstlisting}[language=SQL, caption=MERGE INTO for Postgres extraction]
MERGE INTO glue_catalog.postgres.device t
USING merge_source s
ON t.id = s.id
WHEN MATCHED AND (t.id_firmware <> s.id_firmware OR t.board_serial <> s.board_serial OR t.id_board_model <> s.id_board_model OR t.last_ip <> s.last_ip OR t.city <> s.city OR t.connection_kind <> s.connection_kind OR t.bridge_firm <> s.bridge_firm OR t.cloud_pin <> s.cloud_pin OR t.mirror <> s.mirror) THEN
    UPDATE SET *
WHEN NOT MATCHED THEN
    INSERT *
\end{lstlisting}

The \texttt{MERGE INTO} operation compares the rows from the source table (designated as \texttt{s}) with the target table (designated as \texttt{t}). If a matching row is found (based on the \texttt{id}), and any of the key fields (such as firmware, board serial, or IP address) has changed, the row in the target table is updated. If no matching row is found, the new row from the source table is inserted into the target. This ensures that the table in \ac{S3} remains synchronized with the PostgreSQL source, while also maintaining historical data for Time Travel purposes.

\subsection{MongoDB Data}
For the \ac{IoT} data stored in MongoDB, a completely different approach was adopted. \ac{AWS} Glue does not support bookmarks for connections other than \ac{JDBC} and \ac{S3}, which ruled out the possibility of using Glue's native bookmarking functionality. Additionally, several challenges made it impractical to create a custom bookmark system in Glue. One such limitation is that the functions provided by the Glue context do not support \textit{pushdown predicates}, which are conditions applied at the data source level to filter the data returned. Without pushdown predicate support, every time a collection is read, it must be fully imported and then filtered afterward. Importing several terabytes of data with each job execution is clearly unfeasible.

In principle, \texttt{PySpark} provides a \texttt{read()} function that allows passing an aggregation pipeline as a parameter, allowing to drain data. An aggregation pipeline is a MongoDB framework that processes data through multiple stages, each applying specific transformations or filtering conditions to the dataset. While this approach would work for ongoing operational tasks, during the initial ingestion phase, it would still be highly inefficient due to the large data volume. Handling this with multiple aggregation pipelines, each importing a subset of data, would require sequential execution in Glue, as asynchronous operations are not supported in this environment, significantly increasing execution time.

Given these constraints, it was decided to use \ac{AWS} Lambda functions, which offer scalability based on workload, parallel execution, and support for asynchronous queries. \ac{AWS} Lambda is a serverless computing service that runs code in response to events, automatically managing the infrastructure required to execute the code. A notable limitation of Lambda is its maximum execution duration of 15 minutes, which needs to be considered when handling large-scale data processing tasks.

To deploy the Lambda functions, the \texttt{Serverless Framework} was used. This framework enables simplified management of serverless applications by automating the setup, deployment, and scaling of resources, without needing to manually provision or manage servers. The functions were written in Node.js with TypeScript, chosen for its robust asynchronous code handling capabilities and strong type-checking, which enhances code reliability and maintainability.

Two \acp{ORM} were utilized. For PostgreSQL, \texttt{Prisma} was selected to handle bookmarking data, as it allows connections to multiple databases in the same environment. Prisma simplifies data access by generating a type-safe client, which improves both the efficiency and reliability of database interactions. For MongoDB, the original MongoDB driver developed for Node.js was used, offering direct and optimized support for MongoDB's functionalities and data handling requirements.

To interact with \ac{AWS} services, the JavaScript \texttt{\ac{SDK} v3} was utilized. \ac{AWS} \ac{SDK} v3 is the latest version of the Software Development Kit provided by Amazon, specifically designed to facilitate interaction with \ac{AWS} services. It offers modular packages, each dedicated to a specific service, allowing developers to import only the required functionalities. This approach optimizes application performance by reducing the overall bundle size, which is especially beneficial for serverless environments where minimizing execution time and memory usage is crucial.

The architecture for the Lambda functions consists of two TypeScript-based functions: \textit{master} and \textit{worker}. To initiate the data extraction process, the \textit{master} Lambda function is invoked. This function is responsible for identifying which devices have sent data since the previous job execution and for invoking multiple \textit{worker} Lambdas accordingly. The \textit{worker} Lambdas, then, handles the export of new data for the identified devices, allowing the system to bypass the 15-minute execution timeout limitation by distributing the workload across multiple functions. Each \textit{worker} Lambda imports data from a single collection and processes a subset of devices. Figure \ref{fig:lamschema} illustrates the architecture of the Lambda-based solution used for the data ingestion process.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{res/lambda-schema.pdf}
    \caption{Lambdas architecture}
    \label{fig:lamschema}
\end{figure}

To implement custom bookmarking, a dedicated support database was created in an \ac{AWS} \ac{RDS} instance to avoid overloading the production database. This support database includes a table named \texttt{lastts}, designed to track data ingestion progress. The \texttt{lastts} table contains three columns: \texttt{idDevice}, which identifies each specific device, \texttt{collection}, indicating the dataset collection, and \texttt{lastts}, a timestamp indicating the latest ingestion point for the device. Together, \texttt{idDevice} and \texttt{collection} form the primary key, ensuring uniqueness for each device and collection combination.

In addition, the support database includes a table named \texttt{metrics} for tracking processing metrics. Each time a worker completes processing a list of devices, it logs a new entry in the \texttt{metrics} table with details such as \texttt{First device processed}, \texttt{Last device processed}, \texttt{Devices to query}, \texttt{Documents uploaded}, \texttt{Worker computation time}, \texttt{Total runned queries}, \\\texttt{Devices processed}, \texttt{Collection}, and \texttt{Number of errors}. These metrics provide valuable insights into job performance, allowing for monitoring and optimization of the data ingestion pipeline.

\subsubsection{Master Function}

The \textit{master} function begins by querying all devices in the \texttt{device} table to obtain their \texttt{last\_log\_date} timestamps. These timestamps represent the latest data logged by each device, which is essential for determining if any new data needs to be processed. For each collection in the system, the function also retrieves the \texttt{lastts} timestamp for each device from the \texttt{lastts} table. This \texttt{lastts} timestamp indicates the most recent data ingestion point for that collection. If a device's \texttt{lastts} does not exist, or if its \texttt{last\_log\_date} is greater than its \texttt{lastts}, the device is added to an array called \texttt{devicesToQuery}, marking it as ready for further processing.

Once the \texttt{devicesToQuery} array is populated for each collection, the function organizes these devices into batches for parallel processing. The batching is controlled by \texttt{workerSize}, which is calculated based on the total number of devices and the preset number of workers specified in \texttt{workersPerCollection}. This object defines the desired number of workers per collection, with collections containing a higher volume of documents assigned a greater number of workers to manage the load more effectively. Alternatively, it is possible to use the \textit{split mode} option, which instead of batching devices according to a predefined number of workers, specifies a target number of devices per worker, dynamically adjusting the number of workers invoked according to the number of devices identified for processing.

The code snippet below demonstrates the batching and invocation of workers:
    \begin{lstlisting}[language=Python, caption=Device batching]
const workerSize = Math.ceil(devicesToQuery.length / workersPerCollection[operationalMode][collection])
for (let i = 0; i < devicesToQuery.length; i += workerSize)
    invocations.push(invokeWorkerWithTimeout(devicesToQuery.slice(i, i + workerSize), collection))
\end{lstlisting}

The master function manages all worker invocations using \\ \texttt{Promise.allSettled(invocations)}, which executes all promises concurrently and returns an array of results, one for each promise. In JavaScript, a promise represents an operation that will complete asynchronously in the future, either successfully or with an error. Each result from \texttt{allSettled} indicates whether the corresponding promise was fulfilled or rejected. This approach ensures that all worker executions complete, regardless of individual failures, without interrupting the entire process.

The following functions handle each worker invocation:

\begin{lstlisting}[language=Python, caption=\texttt{invokeWorkerWithTimeout} and \texttt{invokeWorker} functions]
async function invokeWorkerWithTimeout(dev: number[], collection: string) {
  const timeoutPromise = new Promise<never>((_, reject) =>
    setTimeout(
      () => reject(new Error(`Timeout for worker ${collection}${dev[0]}`)),
      WORKERS_TIMEOUT,
    ),
  );

  return Promise.race([invokeWorker(dev, collection), timeoutPromise]);
}

async function invokeWorker(dev: number[], collection: string) {
  const params = {
    FunctionName: `datalake-mongo-extractor-${process.env.NODE_ENV}-worker`,
    Payload: JSON.stringify({
      body: {
        devices: JSON.stringify(dev),
        collection: collection,
      },
    }),
  };

  const command = new InvokeCommand(params);
  const response = await lambdaClient.send(command);

  // Handle worker Lambda response
  if (response.StatusCode !== 200) {
    throw new Error(`Invocation failed for a worker`);
  }

  const payload = JSON.parse(new TextDecoder('utf-8').decode(response.Payload));

  if (payload.errorMessage) {
    throw new Error(
      `${payload.errors} error(s) in worker ${payload.id} (${payload.log_stream_name})`,
    );
  }

  return payload;
}
\end{lstlisting}

The \texttt{invokeWorker} function constructs and sends the invocation command for each worker Lambda. If the response \texttt{StatusCode} is not \texttt{200}, an error is thrown, indicating that the invocation failed. Additionally, if there are specific errors reported in the worker's payload, these are also logged as errors.

In this process, three primary types of errors may occur. Each is handled independently by logging the error to avoid affecting other workers or the master function:
\begin{itemize}
    \item \textbf{Worker invocation failure}: If \texttt{InvokeCommand} returns a \texttt{StatusCode} other than \texttt{200}, the invocation did not succeed.
    \item \textbf{Error within the worker}: If the worker encounters any issue, the master function receives a payload containing the number of errors that occurred. Full error details are logged within each individual worker, accessible via \ac{AWS} CloudWatch.
    \item \textbf{Worker timeout}: If a worker does not complete within 15 minutes, the issue is managed by \texttt{Promise.race()} to which two promises are passed: the actual worker invocation (\texttt{invokeWorker}) and a timeout promise. If the worker exceeds the predefined \texttt{WORKERS\_TIMEOUT} duration, the timeout promise rejects, ensuring the master function is aware that the worker did not complete within the expected time.
\end{itemize}

In this setup, all master and worker logs are available in \ac{AWS} CloudWatch, enabling easy access to error details and execution summaries. \texttt{CloudWatch} is an \ac{AWS} tool for monitoring and managing log data from \ac{AWS} services. 

Once all worker responses are received, the master function logs summary statistics for each collection. This includes progress as a percentage of documents uploaded relative to the total documents in each collection. The uploaded document count is calculated by summing the values in the \texttt{docs\_uploaded} field within the \texttt{metrics} table for a specific collection in the support database. The total document count is estimated using the MongoDB driver's \\ \texttt{estimatedDocumentCount()} function.

Since \texttt{variable\_logs\_clean} is a view and does not support direct document counting, progress is calculated based on the percentage of devices processed relative to the total number of devices. The following function, \texttt{countTotalKeys}, counts the already processed devices:

\begin{lstlisting}[language=Python, caption=\texttt{countTotalKeys} function]
async function countTotalKeys() {
  let totalKeyCount = 0;
  let continuationToken;
  let isTruncated = true;

  while (isTruncated) {
    const params: ListObjectsV2CommandInput = {
      Bucket: AWS_CONFIG.TARGET_BUCKET,
      Prefix: 'variable_logs_clean/',
      Delimiter: '/',
      ContinuationToken: continuationToken,
    };

    const data = await s3Client.send(new ListObjectsV2Command(params));

    // Sum KeyCount from the current response
    totalKeyCount += data.KeyCount ?? 0;

    // Handle pagination
    continuationToken = data.NextContinuationToken;
    isTruncated = data.IsTruncated ?? false;
  }

  return totalKeyCount;
}
\end{lstlisting}

This function iterates through paginated \ac{S3} results to count the total keys, providing an approximate measure of processed devices. This estimation is more approximate than document counting and is mainly useful for bulk loads.

Finally, the master function returns a \ac{JSON} response summarizing the number of successfully executed workers and overall progress, completing the orchestration for data processing and upload.

\subsubsection{Worker function}

As previously seen, the master function invokes each worker function with two parameters: the \texttt{collection} to process and a list of \texttt{devices}. The worker function begins by adding all devices to an \textit{async queue} with a concurrency level of 5, meaning that up to five tasks can be executed concurrently. An \textit{async queue} allows tasks to be processed asynchronously, enabling the execution of multiple tasks in parallel. However, it should be noted that, due to Node.js's single-threaded nature, this parallelism is achieved via asynchronous handling rather than true parallel threads.

Each task in the queue processes a single device, querying and loading its data into the data lake in manageable portions. Given the high volume of data, it is necessary to retrieve records in smaller segments. Starting from the earliest available data in 2015, the function performs sequential queries, each covering a 60-day period until it reaches the device's \texttt{last\_log\_date}. Due to \ac{AWS} Lambda's 15-minute maximum execution time, all queries must complete within 10 minutes, leaving a 5-minute buffer to finalize the last query without risking timeout.

Each execution task in the queue involves a \texttt{while} loop, iterating as long as the total runtime is under 10 minutes. In each iteration, the loop defines the query period's boundaries: \texttt{from} and \texttt{to}. The \texttt{from} value is set to the device's \texttt{lastts} if available; otherwise, it defaults to October 2015 (the beginning of MongoDB data storage). The \texttt{to} value is determined as the minimum between \texttt{from + 60 days} and \texttt{last\_log\_date}. If \texttt{from} is greater than or equal to \texttt{to}, the loop ends, indicating all data has been imported for that device.

\textbf{Query Construction} \\
Once the time boundaries are defined, the worker constructs a MongoDB query tailored to the specific \texttt{collection}. Since each collection has its own structure and timestamp field names, a custom query format is created for each one. For instance, in the \texttt{variable\_logs\_clean} collection, the query is structured as follows:

\begin{lstlisting}[language=Python,caption=]
variable_logs_clean: (device, from, to) => ({
    idDevice: device,
    day: {
      $gte: new Date(from.getTime() - (from.getTime() % 21600000)),
      $lte: new Date(to.getTime() - (to.getTime() % 21600000)),
    },
    time: { $gt: from, $lte: to },
})
\end{lstlisting}

This query filters by \texttt{idDevice}, with \texttt{from} and \texttt{to} as the start and end times, respectively. The time field is represented by both day and time, this is because, as we said, \texttt{variable\_logs\_clean} is a view and day is used to exploit the day index in the real \texttt{variable\_logs} collection.

The MongoDB query then runs, with results stored in \texttt{queryResult}. After obtaining the data, it undergoes two main aggregation steps to ensure consistency and format the records properly:

\begin{itemize}
    \item \textbf{Common aggregation}: Applied to all collections, this step moves non-standard fields (those not defined in the Parquet schema for that collection) into a \texttt{payload} field. This standardizes each document by consolidating unusual fields.
    \item \textbf{Variable\_logs\_clean aggregation}: Specific to the \texttt{variable\_logs\_clean} collection, this aggregation combines all measurements with the same timestamp from a single device into one document.
\end{itemize}

\textbf{Data Loading to the Data Lake} \\
Once the documents are ready for data lake ingestion, the worker function performs three crucial steps: updating the \texttt{lastts}, converting \ac{JSON} documents to Parquet format, and uploading the data to an \ac{S3} bucket. It is essential that these three operations are executed robustly, meaning that if any one of them fails, the other steps should not complete. To ensure this robustness, these operations are encapsulated within a transaction managed by Prisma.

In Prisma, \textit{interactive transactions} provide a way to bundle multiple database operations into a single logical unit. Within an interactive transaction, either all operations are successfully completed and committed, or if any operation fails, the transaction is rolled back, undoing any changes made during the transaction. In this context, if either the conversion to Parquet or the upload to \ac{S3} fails, the transaction is not committed, effectively rolling back the \texttt{lastts} update. Consequently, the next execution will attempt to re-import the same data. Additionally, if a transaction-level error occurs, the worker ensures any partially uploaded files are removed, maintaining data consistency.

For the conversion of \ac{JSON} documents to Parquet format, the \texttt{parquetjs}\footnote{\url{https://www.npmjs.com/package/@dsnp/parquetjs}} library was utilized. \texttt{parquetjs} allows for efficient transformation of \ac{JSON} data into Parquet format, a columnar storage format optimized for high-performance querying. The library supports schema definitions, so each collection's schema is specified, ensuring consistent data structure in the output files. Furthermore, both the conversion and upload processes are performed using \textit{streaming}, allowing the function to handle large data volumes without overwhelming memory.

Streaming in Node.js enables data to be processed in chunks, where each data chunk flows continuously from one stage of the pipeline to the next without waiting for the entire dataset. In this implementation, the streaming pipeline begins with a \texttt{Readable} stream that reads data from \texttt{documentsAggregated}, applies the Parquet transformation, and finally streams the transformed data to the \ac{S3} destination. Here is the code that accomplishes this:

\begin{lstlisting}[language=Python]
const destination = new PassThrough();
const reader = Readable.from(documentsAggregated);
const pt = new ParquetTransformer(parquetSchemas[params.body.collection]);

await new Promise<void>((resolve, reject) => {
  pipeline(reader, pt, destination).catch((err) => reject(err));

  const upload = new Upload({
    client: s3Client,
    params: {
      Bucket: AWS_CONFIG.TARGET_BUCKET,
      Key: `${params.body.collection}/${device}/${from.getTime()}.parquet`,
      Body: destination,
      ContentType: 'application/parquet',
    },
  });
  upload
    .done()
    .then((res) => resolve())
    .catch((err) => reject(err));
});
\end{lstlisting}

In this code:
\begin{itemize}
    \item \texttt{destination} is a \texttt{PassThrough} stream that serves as the final output in the pipeline.
    \item \texttt{Readable.from(documentsAggregated)} creates a readable stream from the \ac{JSON} documents, allowing them to be processed sequentially.
    \item \texttt{ParquetTransformer} applies the Parquet schema transformation, converting the \ac{JSON} data into Parquet format in real-time.
    \item \texttt{pipeline()} connects the readable stream, Parquet transformation, and \texttt{destination} stream in a sequence, with errors caught and handled via \texttt{reject}.
\end{itemize}
The transformed data is then uploaded to \ac{S3} using the \ac{SDK}'s \texttt{Upload()} function, which reads from \texttt{destination} and streams data directly to the \ac{S3} bucket, saving memory and speeding up the process.

Once the queue is empty, either due to reaching the time limit or because all devices have been processed, a new entry is added to the \texttt{metrics} table in the support database. This entry records various metrics measured during execution, such as the number of processed devices, total queries executed, and uploaded document count. These metrics are also logged to \ac{AWS} CloudWatch, where they can be monitored for performance analysis and troubleshooting.

Finally, the worker function returns a response to the master function, including the \texttt{statusCode}, the unique Lambda identifier, and any error counts detected during execution.

\section{Data Integration}
As described earlier, the extracted data is directly stored in two different \ac{S3} buckets. \ac{AWS} \ac{S3} is an object storage service that organizes data within containers called \textit{buckets}, each capable of storing virtually unlimited objects. A bucket does not function as a typical directory; rather, each file within \ac{S3} is identified by a unique \textit{key}, which may include prefixes resembling folder structures. In this system, PostgreSQL data is stored in a bucket named \texttt{datalake-postgres}, where each table is assigned a distinct prefix corresponding to its table name. MongoDB data, on the other hand, is stored in a bucket named \texttt{datalake-mongodb}, organized with prefixes based on collection names and device IDs.

As introduced in Section \ref{sec:wholesystem}, the data lake structure is composed of three layers: \textit{raw}, \textit{curated}, and \textit{analytics}. For PostgreSQL data, it was decided to combine the raw and curated layers, as data can be cleansed during extraction through the Glue \ac{ETL} job. Maintaining an additional extraction job and \ac{S3} bucket for raw data was deemed unnecessary, given that PostgreSQL tables are relatively lightweight and do not justify the complexity of a separate raw layer.

For MongoDB, however, an \ac{ETL} job is required to transform data from the raw layer to the curated layer. The structure of this job is similar to that described in Section~\ref{sec:gluejobdesc}, with each collection processed sequentially. It comprises a source node (reading from the raw layer), a transformation node, and a target node (writing to the curated layer). For this transformation, Glue bookmarks are employed to track progress when reading from \ac{S3}. Unlike \ac{JDBC} bookmarks, which record the last processed primary key, \ac{S3} bookmarks store the last modified timestamp of the files read.

Currently, this \ac{ETL} job performs four key functions:
\begin{enumerate}
    \item conversion from Parquet format to Iceberg,
    \item Snappy compression,
    \item partitioning,
    \item and deduplication of the \texttt{alarm} collection.
\end{enumerate}
Transforming MongoDB tables to Iceberg format is beneficial primarily for two reasons: Iceberg enables optimized table storage through table optimizations and enhances partitioning capabilities. Table optimizations are covered in detail in Section \ref{sec:optimizations}, as these are applied directly within the data catalog. Instead, Iceberg's partitioning facilitates effective partitioning based on metadata for \texttt{timestamp} columns, as well as customizable partitioning by year, month, or day.

Partitioning of collections was implemented to improve query performance by reducing the scope of data scanned \cite{ponnusamy2024scalable}. Specifically, all collections are partitioned by \texttt{idDevice}, and larger collections are additionally partitioned by year. By leveraging Iceberg's hidden partitioning, data can be partitioned based on timestamps down to the year, month, or day level. To determine the optimal time granularity for partitioning, several tests were conducted (see Section \ref{sec:partitioningstrategy}), as daily partitioning is not always ideal. While partitioning by day minimizes the amount of data scanned for short-term queries, it significantly increases \ac{I/O} operations for long-term queries. This results in multiple small files for daily partitions, whereas a yearly partition would only require a single file and a single \ac{I/O} access. 

In the context of company data, the daily volume for each device and collection does not justify creating a separate file. Additionally, most queries target extended time periods, so partitioning by year was chosen to minimize \ac{I/O} overhead, thus improving response latency. The downside of this choice is a potential increase in scanned data for certain queries, which may elevate query costs (particularly when using \ac{AWS} Athena, where costs are based on scanned data size).

\textbf{Deduplication of \texttt{alarm} documents} \\
Deduplication of documents in the \texttt{alarm} collection is necessary because \texttt{alarm} is the only collection in the data lake that may undergo updates. Each document in \texttt{alarm} includes fields such as \texttt{id} (document identifier), \texttt{idDevice} (device identifier), \texttt{code} (alarm code), \texttt{activeTS} (alarm activation timestamp), \texttt{resetTS} (alarm reset timestamp), among others. Here, \texttt{resetTS} is always greater than or equal to \texttt{activeTS}, but a document may initially lack a \texttt{resetTS} if the alarm is active when recorded in the database. The document is subsequently updated with a \texttt{resetTS} once the alarm is deactivated. Consequently, documents are added to the data lake if either \texttt{activeTS} or \texttt{resetTS} is greater than the \texttt{lastts}. Depending on the status of these timestamps during data ingestion, three scenarios can arise:

\begin{itemize}
    \item \texttt{activeTS} and \texttt{resetTS} are both greater than \texttt{lastts}: No deduplication is required.
    \item \texttt{activeTS} is greater than \texttt{lastts} and \texttt{resetTS} does not exist: The document will be re-imported when \texttt{resetTS} becomes available, requiring deduplication.
    \item \texttt{activeTS} is less than \texttt{lastts} and \texttt{resetTS} is greater than \texttt{lastts}: The document is imported, but since it is already present in the data lake, deduplication is required.
\end{itemize}

Glue \ac{ETL} manages deduplication through two methods:
\begin{itemize}
    \item For duplicate documents imported within the same job, deduplication is achieved via a \texttt{groupBy} operation in PySpark, aggregating by \texttt{resetTS}:
    \begin{lstlisting}[language=python]
    .groupBy("id", "class", "activationMail", "activeTS", "code", "iddevice", "resetMail", "payload")
    .agg(max("resetTS").alias("resetTS"))
    \end{lstlisting}
    
    \item For duplicate documents imported by different jobs, deduplication is handled using Apache Iceberg's \texttt{MERGE INTO} operation. If a document with the same \texttt{id} already exists in the Iceberg-formatted \ac{S3} target, the row is updated with the latest \texttt{resetTS}:
    \begin{lstlisting}[language=sql]
    MERGE INTO glue_catalog.mongodb.alarm t
    USING merge_source s
    ON t._id = s._id
    WHEN MATCHED THEN
        UPDATE SET t.resetTS = s.resetTS
    WHEN NOT MATCHED THEN
        INSERT *
    \end{lstlisting}
\end{itemize}

Finally, the analytics layer was designed to facilitate specific data aggregations for targeted analyses. A base job and an \ac{S3} bucket were established for this layer, providing flexibility for future recurring analyses. For instance, this layer can support tasks such as joining multiple tables with column-level aggregations.

\section{Data Cataloguing}
The construction of the data lake is completed by cataloging the various tables. The \ac{AWS} Glue Data Catalog serves as a centralized metadata repository that organizes, describes, and indexes datasets stored within the data lake, making them accessible and queryable through services like \ac{AWS} Athena. This cataloging process facilitates data exploration, searchability, and consistency across the data pipeline. For convenience, two distinct catalogs were created in this setup: \texttt{Postgres} and \texttt{Mongodb}, each corresponding to a database within the Glue Data Catalog. Each database contains all the respective tables present in the various data lake layers.

There are two primary methods for creating a catalog (i.e., a database and tables) in Glue:
\begin{itemize}
    \item \textbf{Using the Glue Crawler}: Glue crawlers automate catalog creation by scanning specified data locations, identifying data formats and schemas, and creating or updating tables. Crawlers work by inspecting a dataset, inferring its schema, and periodically refreshing the catalog entries to ensure metadata remains current.
    
    \item \textbf{Direct Table Creation within Glue Jobs}: Glue jobs can automatically create tables in the catalog when writing data to \ac{S3}. This method, triggered at the time of data ingestion, enables Glue to create or update tables based on the data format, schema, and location specified within the job.
\end{itemize}

For the curated and analytics layers, the second method was applied, allowing tables to be cataloged as soon as data is written to \ac{S3}. However, for MongoDB's raw layer, using a crawler is mandatory to capture the unprocessed data format, as direct cataloging through Glue jobs is not supported. Since the raw layer is considered less relevant for analytical use, the curated layer was cataloged instead, eliminating the need to crawl the raw layer tables with each system execution, thereby reducing both time and cost.

Tables stored in \ac{S3} are available in either Parquet or Iceberg formats. In terms of cataloging, the Glue Data Catalog handles both formats similarly, recording schema, partitioning, and metadata, allowing queries to be executed regardless of format.

\textbf{Table Optimization for Iceberg in the Glue Data Catalog} \\
\label{sec:optimizations}
One of the powerful features in the Glue Data Catalog for Iceberg tables is \textit{table optimization} \cite{optimizations}. The most effective optimization, among those offered by Glue, is the \textit{table compaction}. This feature addresses performance and storage efficiency by compacting small files within the table, a process especially useful for managing data updates. Each system execution potentially generates new files as data is added or updated, which could lead to numerous small files. File \textit{compaction} in Iceberg combines these smaller files into larger ones, reducing file fragmentation and optimizing read performance by lowering the number of \ac{I/O} operations needed for queries.

Compaction is particularly advantageous in this context, as it consolidates updated data added with each execution cycle, thus enhancing query performance and minimizing storage costs. This process not only improves the data lake's efficiency but also helps manage the storage footprint of frequently updated collections, making it easier to manage large datasets effectively over time.

\section{Direct Queries}
For on-demand data queries, \ac{AWS} Athena provides a flexible querying tool that supports ANSI \ac{SQL}, enabling users to execute complex operations such as joins, window functions, and array manipulations. Query results can be viewed directly on the console or downloaded as \ac{CSV} files for further use. Athena relies on the Presto and Trino engines, which are optimized for large-scale data processing and integrate optimally with the Glue Data Catalog, making all cataloged data accessible without additional configuration.

The Athena interface displays the tables in the \texttt{Postgres} and \texttt{Mongodb} catalogs, providing the possibility to preview, create, or delete them and run \ac{SQL} queries with ease. Autocomplete suggestions speed up query creation, helping reduce errors, and users can save frequently used queries to streamline repetitive tasks.

A notable advantage of Athena is its integration with Iceberg, which brings version control and efficient update handling directly into the data lake. With Iceberg, Athena enables point-in-time queries that simplify historical data analysis without requiring duplicated data versions.

For more complex data transformations or iterative analyses, PySpark and Spark SQL offer complementary options. These tools enable large-scale distributed processing, allowing users to employ both Python and \ac{SQL} syntax for diverse analytical needs, integrating with both Parquet and Iceberg data formats.

Athena also provides comprehensive query statistics, including execution time, data scanned, and cost metrics. These insights are crucial for query optimization, helping users adjust their queries to improve efficiency and reduce costs in future executions.
\section{Report creation}
For \ac{BI} reporting, Amazon QuickSight has been primarily used to build and publish analyses. Creating an analysis with up-to-date data in QuickSight involves three main steps: preparing the datasets, building the analysis, and publishing it as a dashboard.

In QuickSight, it is not possible to directly use tables cataloged in the Glue Data Catalog; instead, specific datasets must be created. QuickSight distinguishes between \textit{Data Sources} and \textit{Datasets}. A Data Source represents the underlying connection to a specific data repository, such as an \ac{S3} bucket, \ac{RDS} database, or data lake, while a Dataset refers to the actual data used in the analysis, which can integrate one or more data sources. %For instance, a data source may be an \ac{S3} bucket storing transformed MongoDB collections or a PostgreSQL \ac{RDS} instance containing curated tables.

Datasets in QuickSight can undergo several preparatory transformations to align data with analysis requirements. Users can exclude unnecessary fields, edit existing fields, change data types, add calculated fields, and apply filters to refine the data. Datasets can operate in two distinct modes: \textit{Direct Query} or \textit{SPICE} \cite{spice}. In Direct Query mode, each query retrieves data in real-time from the source, whereas SPICE (Super-fast, Parallel, In-memory Calculation Engine) stores a cached version of the dataset, improving performance but requiring periodic refreshes to maintain updated data. To avoid redundancy, further details about SPICE can be referenced in Section \ref{sec:quick}. SPICE datasets need a defined schedule for refresh operations, which can be set as either \texttt{full} or \texttt{incremental}. Incremental refreshes are particularly efficient, as they only update records that have changed since the last refresh. For MongoDB datasets, enabling incremental refresh required adding a processing timestamp to each document during the \ac{ETL} transformation in Glue, which provides the necessary update information for each record.

Deciding whether to store a dataset in SPICE or Direct Query mode requires careful consideration, as SPICE incurs a monthly cost of \$0.38 per GB stored. With storage requirements for SPICE generally up to 5 times higher than for \ac{S3}, and knowing that all 13 collections in the curated layer collectively require around 300GB in \ac{S3}, using SPICE exclusively would cost approximately 570\$. Due to these significant costs, only the most frequently accessed tables are stored in SPICE. The \texttt{variable\_logs\_clean} collection, which would contribute disproportionately to the cost due to its size, is retained in Direct Query mode.

With datasets prepared, analyses can be built. Each analysis in QuickSight can only contain a single dataset, so it is essential that the selected dataset includes all the necessary fields and data. 
In QuickSight, an analysis is structured as a collection of \textit{sheets}, each of which acts like a separate page within the analysis. Each sheet provides a flexible layout where users can position multiple \textit{visuals}, add text boxes, and even insert images to create a cohesive view of the data. Users can freely arrange these elements, allowing for a customized layout where visuals can be displayed side-by-side, stacked, or arranged in grids. Text boxes and images are added to provide context, explanations, or branding, supporting a more comprehensive presentation of data insights. This flexibility helps users organize and present data on different aspects within a single analysis e.g., a sales analysis might have one sheet focused on revenue trends, another on product performance, and a third on regional comparisons.

Within each sheet, visuals play a central role. A \textit{visual} is a chart type selected to represent the data, with options ranging from line and bar charts to scatter plots, pie charts, and heat maps. Each visual type has parameters specific to its format, allowing users to control how the data is represented. Adding data to visuals is simple and intuitive: users can drag and drop columns from the dataset directly into the visual's parameter fields. For example, to create a line chart showing monthly temperature trends, one could drag a \texttt{Date} column to the X-axis and a \texttt{Temperature} column to the Value field. This drag-and-drop interface makes it easy to experiment with different columns and chart types, adapting the visuals to best fit the data and the analysis goals.

QuickSight also supports complex filtering options to control which data is shown in each visual. Filters can be applied at multiple levels of granularity, giving users flexibility in how they present their data. Filters can be applied at three levels:
\begin{itemize}
    \item \textbf{Visual-level filters}: These filters apply only to the data within a specific visual. For instance, if a visual shows cooking programs, a filter can be set to display only the programs of a specific category, such as "Washings".
    \item \textbf{Sheet-level filters}: Filters applied to an entire sheet affect all visuals within that sheet. This is helpful when analyzing a particular subset across multiple visuals on the same sheet. For example, on a sheet with various charts showing sales data, a filter could limit all visuals to data from the last fiscal quarter.
    \item \textbf{Analysis-level filters}: These global filters apply to the entire analysis, affecting every sheet and visual. This approach is useful for setting consistent filter criteria across all sheets, such as showing only data for Gas ovens throughout the analysis.
\end{itemize}

QuickSight also allows users to create \textit{calculated fields}, which generate new data columns based on existing fields. To create a calculated field, users write a formula using QuickSight's built-in functions \cite{functions}, which include mathematical, statistical, and string operations. For example, a calculated field showing the duration of a program could be created using the formula:
\begin{lstlisting}
dateDiff(endTS, startTS,"MI")
\end{lstlisting}
This formula, once defined, becomes a new column available for use in any visual within the dataset, making it easy to integrate custom metrics directly into the analysis.

Once an analysis is complete, it can be published as a \textit{dashboard}, making it available to other stakeholders. Dashboards in QuickSight are interactive, enabling viewers to filter data, view specific metrics, and drill down into key insights. This interactivity makes dashboards a powerful tool for communicating complex data analyses in an accessible format and supports data-driven decision-making by delivering relevant insights to a wide audience.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{res/alarm_summary.pdf}
    \caption{Quicksight Dashboard about Alarm Summary}
    \label{fig:alarmsummary}
\end{figure}

\subsection{Analysis with Generative AI}

Incorporating generative AI into data analysis enhances user accessibility to data insights without requiring specialized data expertise. Amazon QuickSight Q, an innovative feature within \ac{AWS} QuickSight, empowers users to interact with data using natural language queries. By enabling users to ask questions in plain language, Q lowers the technical barriers to data analysis, making advanced insights accessible to non-technical stakeholders and expanding the utility of business intelligence resources.

QuickSight Q integrates with the QuickSight dashboards, allowing users to query directly from dashboards without navigating complex query tools. This capability supports a range of question types, including queries about metrics, historical comparisons, and trend analysis. For example, a user can ask, “How many ovens were produced in the last quarter?” or “How does current year performance compare to last year?” QuickSight Q interprets these queries, executes the necessary computations, and presents results as visualizations or summary metrics directly in the dashboard interface. This minimizes time-to-insight by reducing dependency on \ac{SQL} or other query languages. In addition to this, Q allows the creation and formatting of specific visual elements or calculated fields by means of instructions provided in natural language, thus speeding up and simplifying their management.

Quicksight Q not only offers AI functionality into dashboards but also allows \textit{topics} to be constructed. Topics are structured data models tailored for natural language queries. Unlike dashboards, which present pre-configured visualizations, topics are designed to interpret and respond to dynamic, ad hoc questions by users. Each topic defines a specific dataset, along with custom metadata such as field definitions, synonyms, and data relationships optimized for natural language processing. Administrators configure topics to reflect business-specific terminology and metrics, allowing QuickSight Q to parse queries accurately based on relevant data fields. This setup enables Q to deliver precise responses, making it ideal for scenarios where users need on-demand insights beyond the fixed queries and visualizations typically available in dashboards.

Unfortunately, the cost of using these features is not moderate; in fact, activation of Q requires a fixed \$250 each month in addition to the per-user costs described more precisely in Section \ref{sec:operationalcosts}.

In summary, by implementing Q, this system leverages AI-driven natural language processing to simplify and accelerate data analysis. This promotes a more data-driven culture throughout the organization.

\section{Orchestration and Scheduling}
To fully automatize the data system, orchestrating the execution of each component in a specific sequence is essential. Some services must be triggered at precise times to avoid overlaps, and certain operations need to follow a particular order so that the output of one stage can serve as the input for the next. This orchestration is managed using Amazon's Step Functions, a serverless workflow service that coordinates multiple \ac{AWS} services into a series of steps defined by a state machine.

Step Functions enables complex workflows by chaining together a series of tasks, where each step can be a distinct \ac{AWS} service task, such as a Lambda function or an activity integrated through \ac{API}s. The state machine, defined in \ac{ASL}, specifies the order, conditions, and dependencies between steps, and it allows for retries, branching, parallelization, and error handling. This flexibility is especially advantageous when coordinating components that need to process and transform data sequentially and concurrently.

In this data import process, two separate workflows, one for \textit{bulk load} and one for \textit{operational} load are implemented. These workflows differ significantly in data volume: the bulk load workflow handles large amounts of data quickly, while the operational workflow supports smaller, more regular data imports. To address these requirements, two distinct Step Functions were created.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{res/stepfunctions_graph.png}
    \caption{\texttt{datalake-cold-start} Step Function for Bulk load}
    \label{fig:step}
\end{figure}

The \texttt{datalake-cold-start} Step Function manages the bulk load. It starts with a parallel phase where two branches execute concurrently to optimize processing time. The first branch handles MongoDB data import and the second focuses on PostgreSQL data. In the MongoDB branch, four sequential invocations of the master Lambda function occur, each of which may trigger up to 30 worker Lambdas. Thus, this branch can potentially invoke up to 120 workers to facilitate data extraction. After these workers complete their tasks, a Glue job is launched to transform the extracted data for the curated layer. In the PostgreSQL branch, the Glue job dedicated to extracting PostgreSQL data is executed. 

After the two branches complete, they merge at a decision point, where the system evaluates the progress. If the system has processed over 95\% of the devices, the workflow transitions to the operational Step Function; otherwise, it re-triggers the same bulk load state machine. 

The progress percentage is determined by calculating the ratio of processed devices to the total number of devices for the \texttt{variable\_logs\_clean} collection. This value is computed by the master Lambda, logged in CloudWatch, and included in the Lambda's response under the \texttt{progress} key. Within the workflow, the progress value from the fourth Lambda invocation is retrieved, passed to the MongoDB Glue job, and subsequently used as a parameter throughout the parallel stage, leading up to the decision state. 

If the progress is below 95\%, the initial ingestion phase must continue. The analytics layer Glue job is triggered, followed by a refresh of all QuickSight datasets. At this stage, the \texttt{ListDataSets} \ac{API} in QuickSight retrieves a list of available datasets. This list is processed in a \texttt{Map} state, iterating over each dataset. For each dataset, \texttt{SPICE Choice} evaluates whether it meets specified conditions, such as containing a valid ID, name, and specific keywords corresponding to the datasets requiring a refresh. In this setup, all SPICE datasets must be manually included in these conditions. Only datasets that satisfy the criteria undergo an incremental refresh, with refresh parameters, including timestamp columns, configured directly in QuickSight during dataset setup.

After the QuickSight refresh completes, the state machine executes the \texttt{StartExecution coldStart} state, launching another state machine to continue the cold start phase.

However, if the progress reaches or exceeds 95\%, the system sends a notification via \ac{SNS}. \ac{SNS} is a fully managed messaging service that sends notifications from one publisher to multiple subscribers. In this workflow, an \ac{SNS} message is configured with the subject "Data lake status" and the specific message "End of cold start phase. Starting operational mode". This notification informs by email both myself and the team leader that the bulk load phase has completed and that the system is transitioning to operational mode. Finally, an \ac{API} function call updates an EventBridge Scheduler, which I will discuss in detail in the following section.

\paragraph{The EventBridge Scheduler}
The EventBridge Scheduler was employed to schedule the operational Step Function flow, as continuous data import was not necessary at this stage. During the bulk load phase, the state machine was configured to auto-trigger in cycles without any pause, making a scheduler redundant. However, the operational phase requires a more controlled data import frequency, typically determined by business needs, whether data updates are required hourly, daily, or weekly. In this case, as near real-time updates were not deemed essential, a daily schedule was chosen to balance data currency and cost-effectiveness.

EventBridge automates the execution of workflows by scheduling events based on pre-defined triggers. To set up this scheduler, a \textit{cron expression} was defined, which is a time-based syntax specifying when the event should trigger. The cron expression used in this case, \texttt{0 1/2 * * ? *}, instructs EventBridge to run the operational workflow every day at a specific hour, giving fine control over execution timing. This cron pattern consists of five fields that represent minute, hour, day of month, month, and day of week values. 

When initially configuring the EventBridge event, it was set to \texttt{disabled}. The final task within the bulk load Step Function enables this event, activating the daily schedule. Once activated, the scheduler initiates the operational Step Function as described later, automatically running the workflow on a daily basis. The scheduler settings, including the cron expression, can be modified or temporarily disabled if business requirements change.

\paragraph{Operational Step Function}
The \textit{operational Step Function} closely resembles the \texttt{cold-start} workflow in structure, with two key differences. Firstly, in the operational phase, only a single invocation of the Lambda function is required to handle MongoDB data import, as one execution is sufficient to process the data accumulated over a single day. This simplification reduces system overhead while keeping the data imports timely.

Secondly, the operational workflow omits the progress check. Once the system transitions to the operational phase, reverting to a previous stage is unnecessary, and thus, tracking the progress of data loading becomes redundant. Additionally, there is no need for the Step Function to trigger itself upon completion, as the EventBridge Scheduler handles regular daily execution.